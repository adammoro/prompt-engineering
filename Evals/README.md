# Evals Framework Sandbox

This is very WIP. It's a place for me to get up to speed with OpenAI's eval framework, in public.

The eval.py file prompts a specified model in config.py with the test cases and compares its responses against the controls. The config.py file is where you can set things like the model you want to test and your API key. The `test_cases` folder has a CSV file for each evaluation topic. 

## Test Cases

Test questions and prompts designed to evaluate an AI's current performance on the given topic.

By assessing the AI's responses to these questions and prompts, you can evaluate its understanding of the given topic and identify areas for improvement in terms of accuracy, clarity, and relevance.
